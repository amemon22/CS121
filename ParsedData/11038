CS 274A: Background Notes and Reading

Notation GuideNotes to Accompany Lectures (Note Sets 1 to 3 are particularly relevant for the 1st and 2nd week)Note Set 1: Review of Probability (PDF)Note Set 2: Multivariate Probability Models (PDF)Note Set 3: Models, Parameters, and Likelihood (PDF) Note Set 4: Mixture Models and the EM Algorithm (PDF)Overview Articles on Probabilistic LearningModel-based machine learning, Chris Bishop, Phil Trans R. Soc. A, 2012. A well-written overview article that reviews some of the key ideas behind probabilistic model-based learningGeneral Background/Review Material on ProbabilityMaterial: random variables, conditional and joint
probabilities, Bayes rule, law of total probability, chain rule
and factorization. Frequentist and Bayesian views of probability. Sets of random variables, the
multivariate Gaussian model. Conditional independence and graphical
models. Note Sets 1 and 2 above Barber
text: pages 1 to 14 (basic probability), pages 29 to 40 (graphical
models), sections 8.1 to 8.4 (univariate and multivariate
distributions) Murphy text: Chapter 1 (introduction) and Chapter 2.1 through 2.5  (probability and distributions)Excellent 15 minute video on multivariate Gaussian distributions from Alex IhlerProbability: The Analysis of Data, vol 1, by Guy Lebanon (html version freely available online - useful as a reference)Learning from Data using Maximum LikelihoodMaterial:  Concepts of models and
          parameters. Definition of the likelihood
          function and the principle of maximum likelihood parameter
          estimation. Using maximum likelihood methods to learn the parameters of Gaussian models, binomial, multivariate and other
          parametric models.Note Set 3 aboveBarber: pages 174-177 Tutorial          paper on maximum likelihood estimation Bayesian LearningMaterial: General
          principles of Bayesian
          estimation: prior densities, posterior densities, MAP, fully Bayesian approaches. Beta/binomial and Gaussian examples. Predictive densities, model selection, model averaging Note Sets 1 and 2 above Barber text: pages 191-194 (in Chapter 9, Learning as Inference), pPages 177-179 in Chapter 8, and Chapter 12 on Bayesian Model SelectionMurphy text: Chapter 3.1 to 3.4 and Chapter 5.1, 5.2, 5.3Chapter on Model Comparison and Occam's Razor from David MacKay's book on Information Theory and Inference, and video lecture of David lecturing on Bayesian inference.   Regression ModelsMaterial: Linear models.
          Normal equations. Systematic and stochastic components. 
 Parameter estimation methods for
          regression. Maximum likelihood and Bayesian interpretations. Barber text: pages 345-350 and Pages 367-374 (in Chapter 17 on Linear Models)Murphy text: Chapter 7.1, 7.2, 7.3 and 7.6  pages 1 to 33 of a classic paper on the bias/variance tradeoff  Mike Tipping's review paper on
          Bayesian regression Slides from Stephen Wright on optimization techniques for machine learning algorithmsProbabilistic ClassificationMaterial: 
          Bayes rule, classification boundaries, discriminant
          functions, 
        Optimal decisions, Bayes error
            rate, Gaussian classifiers.  Likelihood-based approaches and properties of objective functions. Logistic regression and
      neural network models.   Barber text: pages 229-234 (in Chapter 10 on Naive Bayes), pages 353-358 on logistic regression (in Chapter 17 on Linear Models)Murphy text: pages 101-107 on Gaussian classifiers in Chapter 4, Chapter 8.1. 8.2, 8.3  Notes on logistic regression from Charles ElkanGenkin et al, Logistic regression for high-dimensional text data  Probabilistic ClusteringMaterial: Mixtures of
      Gaussians and the associated EM algorithm.  K-means
        clustering. Mixtures of
        conditional indepedence models.  Applications to text
      data. Underlying theory of the EM algorithm.Note Set 4 above (EM for Gaussian mixture models)General derivation of the EM Algorithm: pages 404-406 in Barber, pages 363-369 in MurphyBarber text: pages 403-416 Murphy text: pages 337-356 (Chapter 11)  Jeff Bilmes tutorial
        notes on EM, Frank Dellaert's tutorial
      notes on EMLiang and Klein's Online EM with applications to textFraley and Raftery paper on
          model-based clustering State-Space ModelsMaterial:
discrete and continuous latent-state space models. Hidden Markov
models, Kalman filters. Basic principles of smoothing and filtering.
Parameter estimation methods using EM. Barber text: pages 451-471 (in Chapter 23 on Dynamical Models)Murphy text: Chapter 17.1 to 17.5Sampling MethodsMaterial: Importance sampling, Gibbs sampling, and related ideas Barber text: pages 543-553 (in Chapter 27 on Sampling)   